import os
import torch
from fastapi import BackgroundTasks
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# Config ve Servis ImportlarÄ±
# DÄ°KKAT: Ä°ki ayrÄ± prompt yolu import edildi
from app.core.config import CHAT_MODEL, CHROMA_PATH, LOCAL_EMBEDDING_PATH, DATA_PATH, APP_LINKS, PROMPT_FAST_PATH, PROMPT_THINKING_PATH
from app.services.pdf_loader import load_pdfs_text_only, load_single_pdf
from app.services.logging_service import log_conversation
from app.services.settings_service import get_current_model

# Global DeÄŸiÅŸkenler
rag_chain_fast = None      # HÄ±zlÄ± Mod Zinciri
rag_chain_thinking = None  # DÃ¼ÅŸÃ¼nen Mod Zinciri
retriever = None
vectorstore = None
embeddings = None
current_active_model = None

def load_prompt_from_file(file_path, default_text):
    """
    Belirtilen dosyadan prompt metnini okur.
    Dosya yoksa varsayÄ±lan metni hem dÃ¶ner hem de dosyayÄ± oluÅŸturur.
    """
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    print(f"âœ… Prompt yÃ¼klendi: {os.path.basename(file_path)}")
                    return content
        except Exception as e:
            print(f"âŒ Hata ({file_path}): {e}")
    
    # Dosya yoksa varsayÄ±lanÄ± oluÅŸtur (Admin panelinde boÅŸ gÃ¶rÃ¼nmesin diye)
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(default_text)
    except: pass
    
    return default_text

def initialize_rag():
    global rag_chain_fast, rag_chain_thinking, retriever, vectorstore, embeddings, current_active_model
    
    # GÃ¼ncel modeli ayardan oku
    selected_model = get_current_model()
    current_active_model = selected_model
    
    print(f"ğŸ”„ RAG Sistemi baÅŸlatÄ±lÄ±yor... Model: {selected_model}")

    # --- 1. DONANIM KONTROLÃœ ---
    device = "cuda" if torch.cuda.is_available() else "cpu"
    gpu_name = torch.cuda.get_device_name(0) if device == "cuda" else "Ä°ÅŸlemci"
    print(f"ğŸš€ DONANIM: {gpu_name} (ID: {device.upper()}) AKTÄ°F.")

    # --- 2. EMBEDDING MODELÄ° ---
    print("ğŸ“š Embedding modeli yÃ¼kleniyor...")
    # HuggingFaceEmbeddings kullanÄ±yoruz (Yerel KlasÃ¶rden)
    model_kwargs = {'device': device}
    encode_kwargs = {'normalize_embeddings': True, 'batch_size': 32}
    
    embeddings = HuggingFaceEmbeddings(
        model_name=LOCAL_EMBEDDING_PATH,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # --- 3. VEKTÃ–R VERÄ°TABANI ---
    if not os.path.exists(CHROMA_PATH):
        print(f"ğŸ“‚ VeritabanÄ± ({CHROMA_PATH}) bulunamadÄ±, sÄ±fÄ±rdan oluÅŸturuluyor...")
        docs = load_pdfs_text_only(DATA_PATH)
        if docs:
            splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200, length_function=len)
            chunks = splitter.split_documents(docs)
            vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=CHROMA_PATH)
            print(f"âœ… {len(chunks)} parÃ§a bilgi veritabanÄ±na iÅŸlendi.")
        else:
            print("âš ï¸ UYARI: KlasÃ¶rde okunacak PDF bulunamadÄ±. BoÅŸ veritabanÄ± oluÅŸturuluyor.")
            vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)
    else:
        print(f"ğŸ’¾ Mevcut veritabanÄ± yÃ¼kleniyor: {CHROMA_PATH}")
        vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    # --- 4. LLM AYARLARI ---
    print(f"ğŸ¤– Sohbet Modeli: {selected_model}")
    llm = OllamaLLM(
        model=selected_model,
        temperature=0.1,
        num_gpu=-1,       
        num_ctx=4096,     
        num_thread=8      
    )

    # --- 5. PROMPTLAR (Ä°KÄ° AYRI MOD Ä°Ã‡Ä°N) ---
    
    # A. HÄ±zlÄ± Mod VarsayÄ±lanÄ±
    default_fast = """Sen kurumsal bir asistansÄ±n. GÃ¶revin sadece bilgi vermektir.
    Sadece aÅŸaÄŸÄ±daki 'BaÄŸlam' iÃ§indeki bilgileri kullan.
    Cevaba doÄŸrudan baÅŸla. KÄ±sa, net ve Ã¶z ol.
    
    BaÄŸlam: {context}
    Soru: {question}
    Cevap:"""

    # B. DÃ¼ÅŸÃ¼nen Mod VarsayÄ±lanÄ±
    default_thinking = """Sen kÄ±demli bir analist ve kurumsal danÄ±ÅŸmansÄ±n.
    GÃ¶revin:
    1. AÅŸaÄŸÄ±daki 'BaÄŸlam' bilgisini detaylÄ±ca analiz et.
    2. Soruyu cevaplamadan Ã¶nce, baÄŸlamdaki bilgilerin soruyla iliÅŸkisini kur.
    3. AdÄ±m adÄ±m dÃ¼ÅŸÃ¼n ve detaylÄ±, kapsamlÄ± bir aÃ§Ä±klama yap.
    4. EÄŸer varsa, prosedÃ¼rleri madde madde aÃ§Ä±kla.

    BaÄŸlam (DokÃ¼manlar):
    {context}

    Soru:
    {question}

    DetaylÄ± Analiz ve Cevap:"""

    # Dosyalardan YÃ¼kle
    text_fast = load_prompt_from_file(PROMPT_FAST_PATH, default_fast)
    text_thinking = load_prompt_from_file(PROMPT_THINKING_PATH, default_thinking)

    prompt_fast = ChatPromptTemplate.from_template(text_fast)
    prompt_thinking = ChatPromptTemplate.from_template(text_thinking)

    # --- 6. ZÄ°NCÄ°RLERÄ° OLUÅTUR ---
    
    # Zincir 1: HÄ±zlÄ± (Fast)
    rag_chain_fast = (
        {
            "question": lambda x: x["question"],
            "context": lambda x: _get_context_with_links(x["question"])
        } 
        | prompt_fast 
        | llm 
        | StrOutputParser()
    )

    # Zincir 2: DÃ¼ÅŸÃ¼nen (Thinking)
    rag_chain_thinking = (
        {
            "question": lambda x: x["question"],
            "context": lambda x: _get_context_with_links(x["question"])
        } 
        | prompt_thinking 
        | llm 
        | StrOutputParser()
    )
    
    print("âš¡ RAG Sistemi HazÄ±r (Ã‡ift Modlu)!")

def _get_context_with_links(query):
    # Link Enjeksiyonu
    injected_links = ""
    query_lower = query.lower()
    found_links = []
    
    for app_name, link in APP_LINKS.items():
        if app_name in query_lower:
            found_links.append(f"- {app_name.upper()} EriÅŸim Linki: {link}")
    
    if found_links:
        injected_links = "\n\n[SÄ°STEM TARAFINDAN BULUNAN ERÄ°ÅÄ°M LÄ°NKLERÄ°]:\n" + "\n".join(found_links) + "\n(KullanÄ±cÄ±ya bu linki vererek cevapla.)\n"

    # PDF AramasÄ±
    docs = retriever.invoke(query)
    
    # Debug Ã‡Ä±ktÄ±sÄ±
    print("\n" + "="*40)
    print(f"ğŸ” SORU: {query}")
    if found_links: print(f"ğŸ”— BULUNAN LÄ°NKLER: {found_links}")
    print(f"ğŸ“„ PDF PARÃ‡ASI: {len(docs)}")
    for i, doc in enumerate(docs):
        src = os.path.basename(doc.metadata.get('source', 'Bilinmiyor'))
        print(f"   [{i+1}] {src}")
    print("="*40 + "\n")

    pdf_context = "\n\n".join([d.page_content for d in docs])
    return pdf_context + injected_links

# --- ADMIN: CANLI BELGE EKLEME ---
def ingest_new_file(file_path):
    global vectorstore, embeddings
    if not vectorstore: initialize_rag()

    print(f"ğŸ”„ Yeni dosya iÅŸleniyor: {file_path}")
    new_docs = load_single_pdf(file_path)
    
    if new_docs:
        splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200, length_function=len)
        chunks = splitter.split_documents(new_docs)
        vectorstore.add_documents(chunks)
        print(f"âœ… Eklendi.")
        return True
    return False

# --- KULLANICI: CEVAP ÃœRETME (MOD SEÃ‡Ä°MLÄ°) ---
async def get_answer(query: str, mode: str, ip_address: str, background_tasks: BackgroundTasks):
    """
    Mode parametresine gÃ¶re ('fast' veya 'thinking') ilgili zinciri Ã§alÄ±ÅŸtÄ±rÄ±r.
    """
    if not rag_chain_fast: return "Sistem hazÄ±rlanÄ±yor..."
    
    # Zincir SeÃ§imi
    if mode == "thinking":
        chain = rag_chain_thinking
        log_context = "PDF (Thinking Mode)"
    else:
        chain = rag_chain_fast
        log_context = "PDF (Fast Mode)"
    
    # Ã‡alÄ±ÅŸtÄ±r
    response = await chain.ainvoke({"question": query})
    
    # Asenkron Loglama
    background_tasks.add_task(
        log_conversation,
        query=query,
        response=response,
        context=log_context, 
        model=current_active_model,
        ip_address=ip_address
    )
    
    return response